{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azicon/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/Users/azicon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:27: DeprecationWarning: use options instead of chrome_options\n",
      "2 extra bytes in post.stringData array\n",
      "/Users/azicon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:182: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "/Users/azicon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:186: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'infoLst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2c40a35909f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m \u001b[0mdf_douban\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfoLst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0mdf_douban\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/douban_data_raw.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf_8_sig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'infoLst' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import simplejson as json\n",
    "from datetime import datetime\n",
    "from fontTools.ttLib import TTFont\n",
    "from PIL import ImageFont, Image, ImageDraw, ImageOps\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "browserOptions = Options()\n",
    "\n",
    "capa = DesiredCapabilities.CHROME\n",
    "capa[\"pageLoadStrategy\"] = \"none\"\n",
    "capa[\"goog:loggingPrefs\"] = {\"performance\": \"ALL\"}\n",
    "driver = webdriver.Chrome(desired_capabilities=capa, chrome_options=browserOptions)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "#create snapshot of the entire page to prevent it from constantly changing\n",
    "driver.get(\"https://piaofang.maoyan.com/dashboard/movie\")\n",
    "test = None\n",
    "while not test:\n",
    "    try:\n",
    "        test = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'moviename-td')))\n",
    "    except:\n",
    "        driver.refresh();\n",
    "\n",
    "now = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") # get exact datetime at the time of scrape\n",
    "os.mkdir(\"logs/\" + now)\n",
    "\n",
    "logs_raw = driver.get_log(\"performance\")\n",
    "logs = [json.loads(lr[\"message\"])[\"message\"] for lr in logs_raw]\n",
    "\n",
    "def log_filter(log_):\n",
    "    return (\n",
    "        # is an actual response\n",
    "        log_[\"method\"] == \"Network.responseReceived\"\n",
    "        # and json\n",
    "        and \"json\" in log_[\"params\"][\"response\"][\"mimeType\"]\n",
    "    )\n",
    "\n",
    "responses = []\n",
    "\n",
    "for log in filter(log_filter, logs):\n",
    "    request_id = log[\"params\"][\"requestId\"]\n",
    "    resp_url = log[\"params\"][\"response\"][\"url\"]\n",
    "    response = driver.execute_cdp_cmd(\"Network.getResponseBody\", {\"requestId\": request_id})\n",
    "    responses.append(response)\n",
    "\n",
    "driver.close()\n",
    "    \n",
    "# Get this instance's font file from backend server\n",
    "body0 = json.loads(responses[0]['body'])\n",
    "movieList = body0['movieList']['list']\n",
    "date = body0['calendar']['today']\n",
    "font_url = body0['fontStyle'].split('\"')[-2]\n",
    "\n",
    "# Get reference fonts from the file tree\n",
    "headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "              \"Chrome/66.0.3359.139 Safari/537.36 \"\n",
    "    }\n",
    "\n",
    "woff_url = 'http:' + font_url\n",
    "response_woff = requests.get(woff_url, headers=headers).content\n",
    "\n",
    "with open('temp/fonts.woff', 'wb') as f:\n",
    "    f.write(response_woff)\n",
    "    \n",
    "\n",
    "\n",
    "def uniToHex(uni):\n",
    "    return \"&#x\" + uni[3:].lower()\n",
    "\n",
    "def uni_2_png_stream(txt: str, font: str, img_size=512, font_size=0.7, invert=False):\n",
    "    img = Image.new('1', (img_size, img_size), 255) \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(font, int(img_size * font_size))\n",
    "    \n",
    "    txt = chr(txt)\n",
    "    x, y = draw.textsize(txt, font=font) \n",
    "    draw.text(((img_size - x) // 2, (img_size - y) // 2), txt, font=font, fill=0)\n",
    "    if invert:\n",
    "        img = img.convert('L')\n",
    "        img = ImageOps.invert(img)\n",
    "        img = img.convert('1')\n",
    "    #img.save(txt + '.png')\n",
    "    return img \n",
    "\n",
    "def predict_neural(unicode, fontFile):\n",
    "    image = uni_2_png_stream(int(unicode[3:], 16), fontFile, img_size=28, font_size=0.5, invert=True)\n",
    "    image.save(str(unicodeToInt[unicode]) + '_neuro.png')\n",
    "    matrix_form = np.array(image)\n",
    "    weighted_predictions = np.ndarray.flatten(neural_network.run(matrix_form))\n",
    "    most_possible = np.argmax(weighted_predictions)\n",
    "    return most_possible\n",
    "\n",
    "def predict_tesseract(unicode, fontFile, fontSize=0.5):\n",
    "    image = uni_2_png_stream(int(unicode[3:], 16), fontFile, img_size=1024, font_size=fontSize)\n",
    "    image.save('logs/' + str(now) + '/' + str(unicode) + '.png')\n",
    "    text = pytesseract.image_to_string(image, lang=\"eng\", config=\"--psm 10 outputbase digits -c tessedit_char_whitelist=0123456789\")\n",
    "    return text\n",
    "\n",
    "def predict_tesseract_definite(unicode, fontFile):\n",
    "    result, size = '', 1\n",
    "    while not result and size >= 0:\n",
    "        result = predict_tesseract(x, filename, fontSize=size)\n",
    "        size -= 0.01\n",
    "    return result\n",
    "\n",
    "# Map contours to numbers - the prediction phase may be very slow\n",
    "filename = 'temp/fonts.woff'\n",
    "f = TTFont(filename)\n",
    "hexToInt = {}\n",
    "for x in f.getGlyphNames()[1:-1]:\n",
    "    predict = predict_tesseract_definite(x, filename)\n",
    "    hexToInt[uniToHex(x)] = int(predict)\n",
    "    \n",
    "df = pd.DataFrame.from_records(movieList)\n",
    "unitLookup = {'百': 100, '千': 1000, '万': 10000, '亿': 1*10**8}\n",
    "\n",
    "#converts the weird character to a float\n",
    "def convertToFloat(string):\n",
    "    spCharLst = string.split(';')\n",
    "    result = ''\n",
    "    for i in spCharLst:\n",
    "        if len(i) > 7: #has a dot in front\n",
    "            result += '.' + str(hexToInt[i[1:]])\n",
    "        elif len(i) == 7: #in case of bad parsing\n",
    "            result += str(hexToInt[i])\n",
    "    return float(result)\n",
    "\n",
    "#helper function for converting the entire block to a single int\n",
    "def convertDictToInt(dictionary):\n",
    "    return int(convertToFloat(dictionary['num']) * unitLookup[dictionary['unit']])\n",
    "\n",
    "df['boxSplitUnit'] = df['boxSplitUnit'].apply(convertDictToInt)\n",
    "df['splitBoxSplitUnit'] = df['splitBoxSplitUnit'].apply(convertDictToInt)\n",
    "df['movieInfo'] = df['movieInfo'].apply(lambda x : x['movieName'])\n",
    "df.to_csv(\"logs/\" + now + \"/maoyan_data.csv\", encoding='utf_8_sig')\n",
    "\n",
    "#大盘\n",
    "dapan = pd.DataFrame.from_records(body0['movieList']['nationBoxInfo'])\n",
    "dapan['nationBoxSplitUnit'][0] = convertDictToInt(body0['movieList']['nationBoxInfo']['nationBoxSplitUnit'])\n",
    "dapan['nationSplitBoxSplitUnit'][0] = convertDictToInt(body0['movieList']['nationBoxInfo']['nationSplitBoxSplitUnit'])\n",
    "dapan.drop(labels=['unit'], axis=0, inplace=True)\n",
    "dapan.to_csv(\"logs/\" + now + \"/dapan.csv\", encoding='utf_8_sig')\n",
    "\n",
    "#豆瓣\n",
    "driver = webdriver.Chrome(desired_capabilities=capa, chrome_options=browserOptions)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "driver.get(\"https://movie.douban.com/\")\n",
    "\n",
    "test = None\n",
    "while not test:\n",
    "    try:\n",
    "        test = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'nav')))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "jsonLst = []    \n",
    "soupLst = []\n",
    "percent1star, percent2star, percent3star, percent4star, percent5star = [], [], [], [], []\n",
    "betterThan = []\n",
    "shortReview, reviewRating, helpful, totalReviews = [], [], [], []\n",
    "imdb = []\n",
    "playSource = []\n",
    "\n",
    "def search(name):\n",
    "    inputElement = driver.find_element_by_id('inp-query')\n",
    "    inputElement.send_keys(name)\n",
    "    inputElement.send_keys(Keys.ENTER)\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'cover-link')))\n",
    "    firstMovie = driver.find_elements_by_class_name('cover-link')[0]\n",
    "    firstMovie.click()\n",
    "    res = requests.get(driver.current_url, headers=headers)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "    soupLst.append(soup)\n",
    "    \n",
    "    if len(soup.select('span[class^=\"rating_per\"]')) == 5:\n",
    "        for i, x in enumerate(soup.select('span[class^=\"rating_per\"]')):\n",
    "            try:\n",
    "                globals()['percent' + str(i + 1) + 'star'].append(x.text)\n",
    "            except:\n",
    "                globals()['percent' + str(i + 1) + 'star'].append(None)\n",
    "    else: \n",
    "        for i in range(1, 6):\n",
    "            globals()['percent' + str(i) + 'star'].append(None)\n",
    "            \n",
    "            \n",
    "    try:\n",
    "        playSource.append([x.text.strip() for x in soup.select('a[class^=\"playBtn\"]')])\n",
    "    except:\n",
    "        playSource.append([])\n",
    "            \n",
    "    try:\n",
    "        betterThan.append([x.text for x in soup.select('a[href^=\"/typerank?type_name=\"]')])\n",
    "    except: \n",
    "        betterThan.append([])\n",
    "        \n",
    "    try: \n",
    "        shortReview.append([x.text for x in soup.select('span[class^=\"short\"]')])\n",
    "    except: \n",
    "        shortReview.append([])\n",
    "        \n",
    "    try:\n",
    "        reviewRating.append([x.text for x in soup.select('span[class^=\"votes vote-count\"]')])\n",
    "    except: \n",
    "        reviewRating.append([])\n",
    "        \n",
    "    try:\n",
    "        helpful.append([x['class'][0][-2:-1] for x in soup.select('span[class$=\"0 rating\"]')])\n",
    "    except:\n",
    "        helpful.append([])\n",
    "    \n",
    "    try:\n",
    "        totalReviews.append(soup.select_one('a[href$=\"comments?status=P\"]').text.strip())\n",
    "    except:\n",
    "        totalReviews.append(None)\n",
    "    \n",
    "    try:\n",
    "        imdb.append(re.search('IMDb:</span>(.*)<br/>', str(soup.select_one('div[id^=\"info\"]')), re.IGNORECASE).group(1).strip())\n",
    "    except:\n",
    "        imdb.append(None)\n",
    "        \n",
    "    sj = json.loads(soup.select_one('script[type^=\"application/ld+json\"]').text, strict=False)\n",
    "    jsonLst.append(sj)\n",
    "\n",
    "for i in df['movieInfo']:\n",
    "    search(i)\n",
    "    \n",
    "driver.close()\n",
    "\n",
    "df_douban = pd.DataFrame.from_records(jsonLst)\n",
    "df_douban.to_csv(\"logs/\" + now + \"/douban_data_raw.csv\", encoding='utf_8_sig')\n",
    "\n",
    "def parsePeopleLst(lst):\n",
    "    result = []\n",
    "    for i in lst: \n",
    "        result.append(i['name'])\n",
    "    return result\n",
    "\n",
    "df_combined = df\n",
    "df_combined['imdb'] = imdb\n",
    "df_combined['duration'] = df_douban['duration']\n",
    "df_combined['datePublished'] = df_douban['datePublished']\n",
    "df_combined['genre'] = df_douban['genre']\n",
    "\n",
    "for i in range(1, 6):\n",
    "    df_combined['ratingPercentage' + str(i) + 'Star'] = globals()['percent' + str(i) + 'star']\n",
    "    \n",
    "df_combined['betterThan'] = betterThan\n",
    "df_combined['shortReview'], df_combined['reviewRating'], df_combined['helpful'], df_combined['totalReviews'] = shortReview, reviewRating, helpful, totalReviews\n",
    "\n",
    "df_combined['playSources'] = playSource\n",
    "\n",
    "df_combined['director'] = df_douban['director'].apply(parsePeopleLst)\n",
    "df_combined['author'] = df_douban['author'].apply(parsePeopleLst)\n",
    "df_combined['actors'] = df_douban['actor'].apply(parsePeopleLst)\n",
    "df_combined['description'] = df_douban['description']\n",
    "df_combined['url'] = df_douban['url']\n",
    "df_combined['doubanDataRaw'] = soupLst\n",
    "\n",
    "df_combined.to_csv(\"logs/\" + now + \"/combined.csv\", encoding='utf_8_sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchPathError",
     "evalue": "/.git",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchPathError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-57876031df60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0morigin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mgit_push\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-57876031df60>\u001b[0m in \u001b[0;36mgit_push\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgit_push\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrepo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_OF_GIT_REPO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mrepo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMMIT_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/git/repo/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, odbt, search_parent_directories, expand_vars)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchPathError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m## Walk up the path to find the `.git` dir.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchPathError\u001b[0m: /.git"
     ]
    }
   ],
   "source": [
    "from git import Repo\n",
    "\n",
    "now = \"test\"\n",
    "PATH_OF_GIT_REPO = r'/.git'  # make sure .git folder is properly configured\n",
    "COMMIT_MESSAGE = \"update with data from \" + now\n",
    "\n",
    "def git_push():\n",
    "    repo = Repo(PATH_OF_GIT_REPO)\n",
    "    repo.git.add(update=True)\n",
    "    repo.index.commit(COMMIT_MESSAGE)\n",
    "    origin = repo.remote(name='remote')\n",
    "    origin.push()   \n",
    "\n",
    "git_push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
