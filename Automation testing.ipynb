{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/azicon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught https://piaofang.maoyan.com/dashboard-ajax/movie?orderType=0&uuid=aee59939-cc8e-431f-a75c-834e66c97d2b&timeStamp=1647049120458&User-Agent=TW96aWxsYS81LjAgKE1hY2ludG9zaDsgSW50ZWwgTWFjIE9TIFggMTBfMTVfNykgQXBwbGVXZWJLaXQvNTM3LjM2IChLSFRNTCwgbGlrZSBHZWNrbykgQ2hyb21lLzk3LjAuNDY5Mi45OSBTYWZhcmkvNTM3LjM2&index=430&channelId=40009&sVersion=2&signKey=d5de965e81e989cbea1853e25177e4f1\n",
      "Woff retrieval succuessful: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2 extra bytes in post.stringData array\n",
      "/Users/azicon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:193: DeprecationWarning: use options instead of chrome_options\n",
      "/Users/azicon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:215: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "/Users/azicon/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:219: DeprecationWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1bbab4639537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datePublished'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_douban\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datePublished'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genre'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_douban\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'genre'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m \u001b[0mdf_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ratingValue'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ratingCount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bestRating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_combined\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'worstRating'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_douban\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'aggregateRating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparseRatingLst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.proxy import Proxy, ProxyType\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ------------------------------------------------ 猫眼 ---------------------------------------------------\n",
    "browserOptions = Options()\n",
    "browserOptions.add_argument('--ignore-ssl-errors=yes')\n",
    "browserOptions.add_argument('--ignore-certificate-errors')\n",
    "\n",
    "res = ''\n",
    "if not res:\n",
    "    res = json.loads(requests.get('https://www.proxyscan.io/api/proxy?type=https').text)\n",
    "prox = Proxy()\n",
    "prox.proxy_type = ProxyType.MANUAL\n",
    "prox.http_proxy = str(res[0]['Ip']) + \":\" + str(res[0]['Port'])\n",
    "\n",
    "capa = DesiredCapabilities.CHROME\n",
    "capa[\"pageLoadStrategy\"] = \"none\"\n",
    "capa[\"goog:loggingPrefs\"] = {\"performance\": \"ALL\"}\n",
    "prox.add_to_capabilities(capa)\n",
    "driver = webdriver.Chrome(desired_capabilities=capa, chrome_options=browserOptions)\n",
    "wait = WebDriverWait(driver, 60)\n",
    "\n",
    "#create snapshot of the entire page to prevent it from constantly changing\n",
    "driver.get(\"https://piaofang.maoyan.com/dashboard/movie\")\n",
    "test = None\n",
    "while not test:\n",
    "    try:\n",
    "        test = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'moviename-td')))\n",
    "    except:\n",
    "        driver.refresh();\n",
    "\n",
    "now = datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\") # get exact datetime at the time of scrape\n",
    "os.mkdir(\"logs/\" + now)\n",
    "\n",
    "driver.get_screenshot_as_file(\"logs/\" + now + \"/screenshot.png\") # save screenshot to sanity check later\n",
    "\n",
    "logs_raw = driver.get_log(\"performance\")\n",
    "logs = [json.loads(lr[\"message\"])[\"message\"] for lr in logs_raw]\n",
    "\n",
    "def log_filter(log_):\n",
    "    return (\n",
    "        # is an actual response\n",
    "        log_[\"method\"] == \"Network.responseReceived\"\n",
    "        # and json\n",
    "        and \"json\" in log_[\"params\"][\"response\"][\"mimeType\"]\n",
    "    )\n",
    "\n",
    "responses = []\n",
    "\n",
    "for log in filter(log_filter, logs):\n",
    "    request_id = log[\"params\"][\"requestId\"]\n",
    "    resp_url = log[\"params\"][\"response\"][\"url\"]\n",
    "    print(f\"Caught {resp_url}\")\n",
    "    response = driver.execute_cdp_cmd(\"Network.getResponseBody\", {\"requestId\": request_id})\n",
    "    responses.append(response)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Get this instance's font file from backend server\n",
    "body0 = json.loads(responses[0]['body'])\n",
    "movieList = body0['movieList']['list']\n",
    "date = body0['calendar']['today']\n",
    "font_url = body0['fontStyle'].split('\"')[-2]\n",
    "\n",
    "# Get reference fonts from the file tree\n",
    "from fontTools.ttLib import TTFont\n",
    "headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "              \"Chrome/66.0.3359.139 Safari/537.36 \"\n",
    "    }\n",
    "\n",
    "woff_url = 'http:' + font_url\n",
    "response_woff = requests.get(woff_url, headers=headers).content\n",
    "\n",
    "print(\"Woff retrieval succuessful: \" + str(len(response_woff) > 0))\n",
    "\n",
    "with open('temp/fonts.woff', 'wb') as f:\n",
    "    f.write(response_woff)\n",
    "    \n",
    "    \n",
    "driver.close() # we don't need the driver anymore from this point forward\n",
    "\n",
    "\n",
    "from fontTools.ttLib import TTFont\n",
    "from PIL import ImageFont, Image, ImageDraw, ImageOps\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def uniToHex(uni):\n",
    "    return \"&#x\" + uni[3:].lower()\n",
    "\n",
    "def uni_2_png_stream(txt: str, font: str, img_size=512, font_size=0.7, invert=False):\n",
    "    img = Image.new('1', (img_size, img_size), 255) \n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(font, int(img_size * font_size))\n",
    "    \n",
    "    txt = chr(txt)\n",
    "    x, y = draw.textsize(txt, font=font) \n",
    "    draw.text(((img_size - x) // 2, (img_size - y) // 2), txt, font=font, fill=0)\n",
    "    if invert:\n",
    "        img = img.convert('L')\n",
    "        img = ImageOps.invert(img)\n",
    "        img = img.convert('1')\n",
    "    #img.save(txt + '.png')\n",
    "    return img \n",
    "\n",
    "def predict_neural(unicode, fontFile):\n",
    "    image = uni_2_png_stream(int(unicode[3:], 16), fontFile, img_size=28, font_size=0.5, invert=True)\n",
    "    image.save(str(unicodeToInt[unicode]) + '_neuro.png')\n",
    "    matrix_form = np.array(image)\n",
    "    weighted_predictions = np.ndarray.flatten(neural_network.run(matrix_form))\n",
    "    most_possible = np.argmax(weighted_predictions)\n",
    "    return most_possible\n",
    "\n",
    "def predict_tesseract(unicode, fontFile, fontSize=0.5):\n",
    "    image = uni_2_png_stream(int(unicode[3:], 16), fontFile, img_size=1024, font_size=fontSize)\n",
    "    image.save('logs/' + str(now) + '/' + str(unicode) + '.png')\n",
    "    text = pytesseract.image_to_string(image, lang=\"eng\", config=\"--psm 10 outputbase digits -c tessedit_char_whitelist=0123456789\")\n",
    "    return text\n",
    "\n",
    "def predict_tesseract_definite(unicode, fontFile):\n",
    "    result, size = '', 1\n",
    "    while not result and size >= 0:\n",
    "        result = predict_tesseract(x, filename, fontSize=size)\n",
    "        size -= 0.01\n",
    "    return result\n",
    "\n",
    "\n",
    "# Map contours to numbers - the prediction phase may be very slow\n",
    "filename = 'temp/fonts.woff'\n",
    "f = TTFont(filename)\n",
    "hexToInt = {}\n",
    "for x in f.getGlyphNames()[1:-1]:\n",
    "    predict = predict_tesseract_definite(x, filename)\n",
    "    hexToInt[uniToHex(x)] = int(predict)\n",
    "    \n",
    "print(hexToInt)\n",
    "    \n",
    "df = pd.DataFrame.from_records(movieList)\n",
    "\n",
    "unitLookup = {'百': 100, '千': 1000, '万': 10000, '亿': 1*10**8}\n",
    "\n",
    "#converts the weird character to a float\n",
    "def convertToFloat(string):\n",
    "    spCharLst = string.split(';')\n",
    "    result = ''\n",
    "    for i in spCharLst:\n",
    "        if len(i) > 7: #has a dot in front\n",
    "            result += '.' + str(hexToInt[i[1:]])\n",
    "        elif len(i) == 7: #in case of bad parsing\n",
    "            result += str(hexToInt[i])\n",
    "    return float(result)\n",
    "\n",
    "#helper function for converting the entire block to a single int\n",
    "def convertDictToInt(dictionary):\n",
    "    return int(convertToFloat(dictionary['num']) * unitLookup[dictionary['unit']])\n",
    "\n",
    "df['boxSplitUnit'] = df['boxSplitUnit'].apply(convertDictToInt)\n",
    "df['splitBoxSplitUnit'] = df['splitBoxSplitUnit'].apply(convertDictToInt)\n",
    "df['movieInfo'] = df['movieInfo'].apply(lambda x : x['movieName'])\n",
    "df.to_csv(\"logs/\" + now + \"/maoyan_data.csv\", encoding='utf_8_sig')\n",
    "\n",
    "\n",
    "#大盘\n",
    "dapan = pd.DataFrame.from_records(body0['movieList']['nationBoxInfo'])\n",
    "dapan['nationBoxSplitUnit'][0] = convertDictToInt(body0['movieList']['nationBoxInfo']['nationBoxSplitUnit'])\n",
    "dapan['nationSplitBoxSplitUnit'][0] = convertDictToInt(body0['movieList']['nationBoxInfo']['nationSplitBoxSplitUnit'])\n",
    "dapan.drop(labels=['unit'], axis=0, inplace=True)\n",
    "dapan.to_csv(\"logs/\" + now + \"/dapan.csv\", encoding='utf_8_sig')\n",
    "\n",
    "\n",
    "# -------------------------------------------------- 豆瓣 -------------------------------------------------\n",
    "browserOptions = Options()\n",
    "#browserOptions.add_argument(\"--headless\")\n",
    "\n",
    "capa = DesiredCapabilities.CHROME\n",
    "capa[\"pageLoadStrategy\"] = \"none\"\n",
    "capa[\"goog:loggingPrefs\"] = {\"performance\": \"ALL\"}\n",
    "driver = webdriver.Chrome(desired_capabilities=capa, chrome_options=browserOptions)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "driver.get(\"https://movie.douban.com/\")\n",
    "\n",
    "test = None\n",
    "while not test:\n",
    "    try:\n",
    "        test = wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'nav')))\n",
    "    except:\n",
    "        pass\n",
    "        #driver.refresh();\n",
    "\n",
    "jsonLst = []    \n",
    "soupLst = []\n",
    "percent1star, percent2star, percent3star, percent4star, percent5star = [], [], [], [], []\n",
    "betterThan = []\n",
    "shortReview, reviewRating, helpful, totalReviews = [], [], [], []\n",
    "imdb = []\n",
    "playSource = []\n",
    "\n",
    "def search(name):\n",
    "    inputElement = driver.find_element_by_id('inp-query')\n",
    "    inputElement.send_keys(name)\n",
    "    inputElement.send_keys(Keys.ENTER)\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'cover-link')))\n",
    "    firstMovie = driver.find_elements_by_class_name('cover-link')[0]\n",
    "    firstMovie.click()\n",
    "    res = requests.get(driver.current_url, headers=headers)\n",
    "    soup = bs(res.text, 'lxml')\n",
    "    soupLst.append(soup)\n",
    "    \n",
    "    if len(soup.select('span[class^=\"rating_per\"]')) == 5:\n",
    "        for i, x in enumerate(soup.select('span[class^=\"rating_per\"]')):\n",
    "            try:\n",
    "                globals()['percent' + str(i + 1) + 'star'].append(x.text)\n",
    "            except:\n",
    "                globals()['percent' + str(i + 1) + 'star'].append(None)\n",
    "    else: \n",
    "        for i in range(1, 6):\n",
    "            globals()['percent' + str(i) + 'star'].append(None)\n",
    "            \n",
    "            \n",
    "    try:\n",
    "        playSource.append([x.text.strip() for x in soup.select('a[class^=\"playBtn\"]')])\n",
    "    except:\n",
    "        playSource.append([])\n",
    "            \n",
    "    try:\n",
    "        betterThan.append([x.text for x in soup.select('a[href^=\"/typerank?type_name=\"]')])\n",
    "    except: \n",
    "        betterThan.append([])\n",
    "        \n",
    "    try: \n",
    "        shortReview.append([x.text for x in soup.select('span[class^=\"short\"]')])\n",
    "    except: \n",
    "        shortReview.append([])\n",
    "        \n",
    "    try:\n",
    "        reviewRating.append([x.text for x in soup.select('span[class^=\"votes vote-count\"]')])\n",
    "    except: \n",
    "        reviewRating.append([])\n",
    "        \n",
    "    try:\n",
    "        helpful.append([x['class'][0][-2:-1] for x in soup.select('span[class$=\"0 rating\"]')])\n",
    "    except:\n",
    "        helpful.append([])\n",
    "    \n",
    "    try:\n",
    "        totalReviews.append(soup.select_one('a[href$=\"comments?status=P\"]').text.strip())\n",
    "    except:\n",
    "        totalReviews.append(None)\n",
    "    \n",
    "    try:\n",
    "        imdb.append(re.search('IMDb:</span>(.*)<br/>', str(soup.select_one('div[id^=\"info\"]')), re.IGNORECASE).group(1).strip())\n",
    "    except:\n",
    "        imdb.append(None)\n",
    "        \n",
    "    sj = json.loads(soup.select_one('script[type^=\"application/ld+json\"]').text, strict=False)\n",
    "    jsonLst.append(sj)\n",
    "\n",
    "for i in df['movieInfo']:\n",
    "    search(i)\n",
    "    \n",
    "df_douban = pd.DataFrame.from_records(jsonLst)\n",
    "df_douban.to_csv(\"logs/\" + now + \"/douban_data_raw.csv\", encoding='utf_8_sig')\n",
    "\n",
    "\n",
    "def parsePeopleLst(lst):\n",
    "    result = []\n",
    "    for i in lst: \n",
    "        result.append(i['name'])\n",
    "    return result\n",
    "\n",
    "def parseRatingLst(lst):\n",
    "    return (lst['ratingValue'], lst['ratingCount'], lst['bestRating'], lst['worstRating'])\n",
    "\n",
    "df_combined = df\n",
    "df_combined['imdb'] = imdb\n",
    "df_combined['duration'] = df_douban['duration']\n",
    "df_combined['datePublished'] = df_douban['datePublished']\n",
    "df_combined['genre'] = df_douban['genre']\n",
    "df_combined['ratingValue'], df_combined['ratingCount'], df_combined['bestRating'], df_combined['worstRating'] = df_douban['aggregateRating'].apply(parseRatingLst)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    df_combined['ratingPercentage' + str(i) + 'Star'] = globals()['percent' + str(i) + 'star']\n",
    "    \n",
    "df_combined['betterThan'] = betterThan\n",
    "df_combined['shortReview'], df_combined['reviewRating'], df_combined['helpful'], df_combined['totalReviews'] = shortReview, reviewRating, helpful, totalReviews\n",
    "\n",
    "df_combined['playSources'] = playSource\n",
    "\n",
    "df_combined['director'] = df_douban['director'].apply(parsePeopleLst)\n",
    "df_combined['author'] = df_douban['author'].apply(parsePeopleLst)\n",
    "df_combined['actors'] = df_douban['actor'].apply(parsePeopleLst)\n",
    "df_combined['description'] = df_douban['description']\n",
    "df_combined['url'] = df_douban['url']\n",
    "df_combined['doubanDataRaw'] = soupLst\n",
    "\n",
    "df_combined.to_csv(\"logs/\" + now + \"/combined.csv\", encoding='utf_8_sig')\n",
    "\n",
    "PATH_OF_GIT_REPO = r'.git'\n",
    "from git import Repo\n",
    "def git_push():\n",
    "    repo = Repo(PATH_OF_GIT_REPO)\n",
    "    repo.git.add(update=True)\n",
    "    repo.index.commit(\"ip pooling capabilities\")\n",
    "    origin = repo.remote(name='remote')\n",
    "    origin.push()   \n",
    "\n",
    "git_push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
